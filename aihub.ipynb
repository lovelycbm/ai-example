{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용자 정의 데이터셋  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropDataset(Dataset):\n",
    "    def __init__(self, json_dir, img_dir, transform=None):\n",
    "        print(f\"Creating dataset from {json_dir} and {img_dir}\")\n",
    "        self.json_dir = json_dir\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.json_files = glob.glob(os.path.join(json_dir, \"*.json\"))\n",
    "        self.data = []\n",
    "        print(f\"Found {len(self.json_files)} json files\")\n",
    "        for json_file in self.json_files:\n",
    "            with open(json_file, 'r', encoding='utf-8-sig') as f:\n",
    "                item = json.load(f)\n",
    "                img_id = item['ID']\n",
    "                img_path = os.path.join(self.img_dir, f\"{img_id}.jpg\")\n",
    "                if os.path.exists(img_path):\n",
    "                    self.data.append(item)\n",
    "                else:\n",
    "                    print(f\"Warning: No matching image for JSON file {json_file}\")\n",
    "        print(f\"Loaded {len(self.data)} valid samples out of {len(self.json_files)} JSON files\")\n",
    "        # for json_file in self.json_files:\n",
    "        #     with open(json_file, 'r', encoding='utf-8-sig') as f:\n",
    "        #         self.data.append(json.load(f))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        img_id = item['ID']\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, f\"{img_id}.jpg\")\n",
    "\n",
    "        # print(img_path)\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            raise FileNotFoundError(f\"No image file found for ID: {img_id}\")\n",
    "        # else:\n",
    "            # print(f\"Searching for image: {img_path}\")            \n",
    "            # print(f\"Current working directory: {os.getcwd()}\")\n",
    "            # print(f\"Contents of image directory:\")\n",
    "            # print(os.listdir(self.img_dir))            \n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        features = torch.tensor([\n",
    "            item['GSD'],\n",
    "            item['LONG'],\n",
    "            item['LAT'],\n",
    "            item['GROWTH_1'],\n",
    "            item['GROWTH_2']\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        yield_ = torch.tensor(item['YIELD'], dtype=torch.float32)\n",
    "\n",
    "        return image, features, yield_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropYieldModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CropYieldModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 28 * 28 + 5, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, img, features):\n",
    "        x = self.pool(self.relu(self.conv1(img)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 28 * 28)\n",
    "        x = torch.cat((x, features), dim=1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리 및 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset from json_directory/ and images_directory/\n",
      "Found 50 json files\n",
      "Loaded 50 valid samples out of 50 JSON files\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 및 로딩\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = CropDataset('json_directory/', 'images_directory/', transform=transform)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델, 손실 함수, 옵티마이저 초기화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CropYieldModel().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 142816.6094\n",
      "Epoch [2/100], Loss: 129073.8047\n",
      "Epoch [3/100], Loss: 51095.5586\n",
      "Epoch [4/100], Loss: 30476.1973\n",
      "Epoch [5/100], Loss: 39896.3320\n",
      "Epoch [6/100], Loss: 23533.9824\n",
      "Epoch [7/100], Loss: 5468.3447\n",
      "Epoch [8/100], Loss: 41826.0000\n",
      "Epoch [9/100], Loss: 18222.2441\n",
      "Epoch [10/100], Loss: 12312.0449\n",
      "Epoch [11/100], Loss: 20045.3340\n",
      "Epoch [12/100], Loss: 13747.0586\n",
      "Epoch [13/100], Loss: 7740.0879\n",
      "Epoch [14/100], Loss: 22451.9434\n",
      "Epoch [15/100], Loss: 10946.6855\n",
      "Epoch [16/100], Loss: 11028.8994\n",
      "Epoch [17/100], Loss: 14664.2744\n",
      "Epoch [18/100], Loss: 14650.8145\n",
      "Epoch [19/100], Loss: 7841.8213\n",
      "Epoch [20/100], Loss: 4693.2178\n",
      "Epoch [21/100], Loss: 1515.0146\n",
      "Epoch [22/100], Loss: 9797.6055\n",
      "Epoch [23/100], Loss: 5721.3516\n",
      "Epoch [24/100], Loss: 6243.1777\n",
      "Epoch [25/100], Loss: 11017.9346\n",
      "Epoch [26/100], Loss: 2067.0542\n",
      "Epoch [27/100], Loss: 4877.7949\n",
      "Epoch [28/100], Loss: 2930.8066\n",
      "Epoch [29/100], Loss: 3953.2661\n",
      "Epoch [30/100], Loss: 4675.0957\n",
      "Epoch [31/100], Loss: 2914.8496\n",
      "Epoch [32/100], Loss: 1629.0719\n",
      "Epoch [33/100], Loss: 5591.6494\n",
      "Epoch [34/100], Loss: 2953.5186\n",
      "Epoch [35/100], Loss: 4110.4409\n",
      "Epoch [36/100], Loss: 2710.8940\n",
      "Epoch [37/100], Loss: 2744.1558\n",
      "Epoch [38/100], Loss: 3370.0115\n",
      "Epoch [39/100], Loss: 3039.5110\n",
      "Epoch [40/100], Loss: 3268.5459\n",
      "Epoch [41/100], Loss: 2156.6619\n",
      "Epoch [42/100], Loss: 2777.3169\n",
      "Epoch [43/100], Loss: 1798.6743\n",
      "Epoch [44/100], Loss: 3749.7761\n",
      "Epoch [45/100], Loss: 2367.1260\n",
      "Epoch [46/100], Loss: 2432.1892\n",
      "Epoch [47/100], Loss: 1442.8926\n",
      "Epoch [48/100], Loss: 4047.5508\n",
      "Epoch [49/100], Loss: 2647.4119\n",
      "Epoch [50/100], Loss: 2667.6208\n",
      "Epoch [51/100], Loss: 2612.4795\n",
      "Epoch [52/100], Loss: 2071.3230\n",
      "Epoch [53/100], Loss: 2559.8755\n",
      "Epoch [54/100], Loss: 1202.6097\n",
      "Epoch [55/100], Loss: 1350.2264\n",
      "Epoch [56/100], Loss: 2778.2607\n",
      "Epoch [57/100], Loss: 3295.9387\n",
      "Epoch [58/100], Loss: 2805.2219\n",
      "Epoch [59/100], Loss: 1866.6315\n",
      "Epoch [60/100], Loss: 1644.8821\n",
      "Epoch [61/100], Loss: 2681.6577\n",
      "Epoch [62/100], Loss: 3053.0063\n",
      "Epoch [63/100], Loss: 1902.5807\n",
      "Epoch [64/100], Loss: 1997.2185\n",
      "Epoch [65/100], Loss: 1414.1642\n",
      "Epoch [66/100], Loss: 1526.3364\n",
      "Epoch [67/100], Loss: 2001.1986\n",
      "Epoch [68/100], Loss: 1124.9388\n",
      "Epoch [69/100], Loss: 2017.8021\n",
      "Epoch [70/100], Loss: 584.5411\n",
      "Epoch [71/100], Loss: 2252.2617\n",
      "Epoch [72/100], Loss: 2245.8987\n",
      "Epoch [73/100], Loss: 3096.8765\n",
      "Epoch [74/100], Loss: 2212.1377\n",
      "Epoch [75/100], Loss: 2150.2676\n",
      "Epoch [76/100], Loss: 982.3313\n",
      "Epoch [77/100], Loss: 1203.2853\n",
      "Epoch [78/100], Loss: 1248.6714\n",
      "Epoch [79/100], Loss: 949.3691\n",
      "Epoch [80/100], Loss: 2202.6421\n",
      "Epoch [81/100], Loss: 738.7863\n",
      "Epoch [82/100], Loss: 1160.6381\n",
      "Epoch [83/100], Loss: 974.1906\n",
      "Epoch [84/100], Loss: 1383.1089\n",
      "Epoch [85/100], Loss: 1550.2609\n",
      "Epoch [86/100], Loss: 1406.3977\n",
      "Epoch [87/100], Loss: 2010.4991\n",
      "Epoch [88/100], Loss: 997.9538\n",
      "Epoch [89/100], Loss: 1299.5612\n",
      "Epoch [90/100], Loss: 1460.9890\n",
      "Epoch [91/100], Loss: 657.3724\n",
      "Epoch [92/100], Loss: 1330.6206\n",
      "Epoch [93/100], Loss: 913.9128\n",
      "Epoch [94/100], Loss: 1076.1302\n",
      "Epoch [95/100], Loss: 432.4182\n",
      "Epoch [96/100], Loss: 1242.7117\n",
      "Epoch [97/100], Loss: 1168.2483\n",
      "Epoch [98/100], Loss: 936.1643\n",
      "Epoch [99/100], Loss: 388.2722\n",
      "Epoch [100/100], Loss: 604.3615\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (images, features, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images, features)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'crop_yield_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Yield: 245.98\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    new_image_id = \"0120200824MS02N000070\"  # 예시 ID\n",
    "    \n",
    "    # 새 JSON 파일 읽기\n",
    "    with open(f\"json_directory/{new_image_id}.json\", 'r', encoding='utf-8-sig') as f:\n",
    "        new_data = json.load(f)\n",
    "    \n",
    "    # 새 이미지 파일 찾기 (jpg만)\n",
    "    new_image_jpg = f\"images_directory/{new_image_id}.jpg\"\n",
    "    \n",
    "    if os.path.exists(new_image_jpg):\n",
    "        new_image_path = new_image_jpg\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No image file found for ID: {new_image_id}\")\n",
    "    \n",
    "    new_image = transform(Image.open(new_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    new_features = torch.tensor([[\n",
    "        new_data['GSD'],\n",
    "        new_data['LONG'],\n",
    "        new_data['LAT'],\n",
    "        new_data['GROWTH_1'],\n",
    "        new_data['GROWTH_2']\n",
    "    ]], dtype=torch.float32).to(device)\n",
    "    \n",
    "    prediction = model(new_image, new_features)\n",
    "    print(f'Predicted Yield: {prediction.item():.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
